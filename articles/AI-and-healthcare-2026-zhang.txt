3:"$Sreact.fragment"
4:I[6498,["177","static/chunks/app/layout-50006a05f4f38e04.js"],"default"]
5:I[8312,["177","static/chunks/app/layout-50006a05f4f38e04.js"],"ThemeProvider"]
6:I[5244,[],""]
7:I[3866,[],""]
9:I[6213,[],"OutletBoundary"]
b:I[6213,[],"MetadataBoundary"]
d:I[6213,[],"ViewportBoundary"]
f:I[4835,[],""]
1:HL["/alpacanotes/_next/static/css/87ceaea6f9666f43.css","style"]
2:HL["/alpacanotes/_next/static/css/b95c8e191d13d635.css","style"]
0:{"P":null,"b":"ELLwxBb0fycUknSCqSKVn","p":"/alpacanotes","c":["","articles","AI-and-healthcare-2026-zhang"],"i":false,"f":[[["",{"children":["articles",{"children":[["id","AI-and-healthcare-2026-zhang","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$3","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/alpacanotes/_next/static/css/87ceaea6f9666f43.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/alpacanotes/_next/static/css/b95c8e191d13d635.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":["$","body",null,{"className":"antialiased min-h-screen","children":[["$","$L4",null,{}],["$","$L5",null,{"attribute":"class","defaultTheme":"system","enableSystem":true,"disableTransitionOnChange":true,"children":["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]}]]}]}]]}],{"children":["articles",["$","$3","c",{"children":[null,["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children","articles","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]]}],{"children":[["id","AI-and-healthcare-2026-zhang","d"],["$","$3","c",{"children":[null,["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children","articles","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]]}],{"children":["__PAGE__",["$","$3","c",{"children":["$L8",null,["$","$L9",null,{"children":"$La"}]]}],{},null]},null]},null]},null],["$","$3","h",{"children":[null,["$","$3","86ISn53XVEqr8u8WAV1m1",{"children":[["$","$Lb",null,{"children":"$Lc"}],["$","$Ld",null,{"children":"$Le"}],null]}]]}]]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
10:I[2647,["935","static/chunks/935-c3c4a42486495e3b.js","30","static/chunks/30-cde6a761eaef6c52.js","598","static/chunks/598-8424641e72451bd5.js","406","static/chunks/app/articles/%5Bid%5D/page-b4c9447121244ee6.js"],"default"]
11:I[7123,["935","static/chunks/935-c3c4a42486495e3b.js","30","static/chunks/30-cde6a761eaef6c52.js","598","static/chunks/598-8424641e72451bd5.js","406","static/chunks/app/articles/%5Bid%5D/page-b4c9447121244ee6.js"],"default"]
14:I[4839,["935","static/chunks/935-c3c4a42486495e3b.js","30","static/chunks/30-cde6a761eaef6c52.js","598","static/chunks/598-8424641e72451bd5.js","406","static/chunks/app/articles/%5Bid%5D/page-b4c9447121244ee6.js"],""]
12:T2b8a,<h1 id="ai与医疗效率与理解力的博弈">AI与医疗：效率与理解力的博弈</h1>
<blockquote>
<p><strong>作者</strong>：<a href="https://x.com/AztecaAlpaca">阿兹特克小羊驼 (@AztecaAlpaca)</a><br>
<strong>发布时间</strong>：2026-01-14<br>
<strong>原文链接</strong>：<a href="https://x.com/AztecaAlpaca/status/2011236247877140631">点击跳转</a></p>
</blockquote>
<p><img src="https://pbs.twimg.com/media/G-j2pjNbgAA_18n?format=jpg&#x26;name=large" alt="封面图"></p>
<h2 id="解析张文宏的去技能化预警与全球医疗系统的应对策略">解析张文宏的“去技能化”预警与全球医疗系统的应对策略</h2>
<p>在1月10日于香港举行的高山书院十周年论坛上，国家传染病医学中心（上海）主任张文宏教授就 AI 在医疗领域的应用分享了见解。</p>
<p><img src="https://pbs.twimg.com/media/G-jnyX9bQAEaYvn?format=jpg&#x26;name=large" alt="张文宏观点"></p>
<p>张文宏明确反对将其系统性地引入医院的日常诊疗流程。</p>
<blockquote>
<p><strong>张文宏解释：</strong>
他个人使用 AI 的方式是让其对病例“先看一遍”，事后凭借深厚的专业经验，他可以一看就知道 AI 哪里是错的。</p>
<p>张文宏担忧的在于，一名医生若从实习阶段就未经完整的诊断思维训练，直接借助 AI 获得结论，将导致其无法鉴别 AI 诊断的正误。这种能力的缺失，是隐藏在技术便利背后的深层隐患。</p>
</blockquote>
<p>张文宏医生是一位话题人物，经常能够抛出值得讨论的问题。这一次，无疑又引起了热议。</p>
<p><img src="https://pbs.twimg.com/media/G-jq6jUbQAMsvjl?format=jpg&#x26;name=large" alt="热议"></p>
<p>在当前，如何将 AI 应用于医疗，始终是个热点话题。首先，繁琐的医疗工作者不可能不用 AI。但医疗又是一个偏保守的行业，至少作为患者的时候，每个人还是希望能够被严肃对待。</p>
<p><img src="https://pbs.twimg.com/media/G-jtYc9bQAMC4y1?format=jpg&#x26;name=large" alt="保守与效率"></p>
<h2 id="医疗中ai-在不同阶段的参与方式与风险评估">医疗中，AI 在不同阶段的参与方式与风险评估</h2>
<p>当我们把人工智能放进医生的诊室时，我们其实是在进行一场巨大的交易。一方面，我们看到了惊人的效率——机器不知疲倦，能处理海量数据；但另一方面，我们面临着一个根本性的隐患：<strong>如果机器替医生做了所有的思考，医生还会思考吗？</strong></p>
<p><img src="https://pbs.twimg.com/media/G-jtG2wWMAAPspm?format=jpg&#x26;name=large" alt="交易与隐患"></p>
<p>这里有几个关键点：</p>
<ol>
<li><strong>认知风险是真实的：</strong> 张文宏医生提出的观点非常犀利。如果你让年轻医生在学会如何独立诊断之前就依赖 AI，你就剥夺了他们“挣扎”的机会。没有这种认知的挣扎，就没有技能的习得。最终，医生可能变成仅仅是给算法盖章的办事员。</li>
<li><strong>但收益也是巨大的：</strong> 我们不能因噎废食。在阅片这种枯燥且容易出错的工作上，AI 是完美的“第二双眼睛”。它还能处理那些让医生精疲力竭的文书工作，把医生还给病人。</li>
<li><strong>规则正在制定：</strong> 无论是美国、欧盟还是亚洲，大家都在做同一件事：给这个强大的工具装上护栏。核心原则很简单——不管机器多么聪明，最终签字负责的必须是人类。</li>
<li><strong>教育必须改变：</strong> 既然世界变了，学校也得变。哈佛和英国的医学院正在教学生如何质疑 AI，甚至强迫学生在“关掉 AI”的情况下进行演练，以确保他们的基本功没有退化。</li>
</ol>
<p>总之，AI 是一把手术刀，行善与作恶就在毫厘之间。在这一行，你必须比手中的工具更聪明，否则你就不是在做手术，而是在制造灾难。</p>
<p>接下来我们将对上述要点进行详细解读，我们需要搞清楚，为什么在医疗中，使用 AI 远比仅仅“使用工具”更复杂。</p>
<h3 id="1-便利是思维的敌人吗">1. 便利是思维的敌人吗？</h3>
<p>这就是我们面临的“认识论危机”。张文宏医生的观点触及了学习的本质。</p>
<p>想象一下学习物理，如果他在学会牛顿定律之前就只会背公式表，他永远成不了物理学家。医学也是如此。张文宏担心的是，如果我们让 AI 系统性地接管诊断流程，年轻医生就会跳过“第一性原理”的推导过程。他们会直接得到答案，而错过了那种构建直觉所必需的“脑力折磨”。</p>
<p>当 AI 直接把结果端上来时，我们的大脑就会停止那种深入的、慢速的分析（系统2思维），转而依赖直觉（系统1思维）。结果？我们得到了一群只会验证答案、不会寻找答案的医生。</p>
<p><img src="https://pbs.twimg.com/media/G-jrR_GbQAAGwcy?format=jpg&#x26;name=large" alt="思维危机"></p>
<h3 id="2-技能退化当大脑进入自动驾驶">2. 技能退化：当大脑进入“自动驾驶”</h3>
<p>这不仅仅是担忧，这已经在发生了。当你不再使用某种肌肉，它就会萎缩；大脑也是一样。</p>
<ul>
<li><strong>静默的自动驾驶：</strong> 医生习惯了 AI 总是对的，于是停止了检查。他们脑中的疾病模型因为长期不用而变得迟钝。</li>
<li><strong>捷径式学习：</strong> 实习生用 AI 读摘要，而不是自己去啃复杂的病历。这就像是只看电影解说而不看电影，你以为你懂了，其实你什么都没懂。</li>
<li><strong>提示词贫乏：</strong> 医生如果只会给 AI 下达简单的指令，他们得到的也只是泛泛的回答。</li>
</ul>
<p><img src="https://pbs.twimg.com/media/G-jrKH9bQAEp-sO?format=jpg&#x26;name=large" alt="自动驾驶风险"></p>
<p>更糟糕的是<strong>自动化偏见</strong>。这是一种心理陷阱：我们倾向于相信屏幕上的东西。即使 AI 犯了一个显而易见的错误，或者漏掉了一个关键信号，医生也可能因为过度信任机器而选择无视自己的直觉。这就像是我们为了省点脑力，就把方向盘交给了并不完美的自动驾驶仪。</p>
<h3 id="3-反方观点我们需要这个增益器">3. 反方观点：我们需要这个“增益器”</h3>
<p>尽管风险存在，但埃里克·托波尔（Eric Topol）等人的观点同样有力：人类本身就是充满缺陷的。我们会累，会看走眼，会有偏见。</p>
<p><img src="https://pbs.twimg.com/media/G-jrgedbQAMKvF8?format=jpg&#x26;name=large" alt="增益器"></p>
<ul>
<li><strong>安全网：</strong> 在放射科，AI 不累。它能发现人类因疲劳而漏掉的肺结节。这不仅是效率，这是救命。</li>
<li><strong>把时间还给医生：</strong> 现在的医生被文书工作淹没了。如果 AI 能处理这些琐事，医生就能去做只有人类能做的事——倾听病人，理解那些数据之外的痛苦。</li>
<li><strong>超越人类的极限：</strong> AI 能看见我们看不见的模式。它能综合一个人一生的健康数据来预测败血症。这不仅仅是辅助，这是超越。</li>
</ul>
<h3 id="4-深入案例ai-写病历的隐形代价">4. 深入案例：AI 写病历的隐形代价</h3>
<p>用 AI 自动生成病历听起来很棒，能省下无数小时。数据也支持这一点：医生轻松了，病人觉得医生更专注了。</p>
<p>但这里有个微妙的陷阱——<strong>“脱钩”</strong>。</p>
<p><img src="https://pbs.twimg.com/media/G-jrq8tbQAAiUxo?format=jpg&#x26;name=large" alt="脱钩"></p>
<p>写病历不仅仅是记录，它其实是医生整理思路、消化信息的过程。如果你把这个过程外包给机器，你可能就失去了对病例深层理解的机会。而且，机器往往会过滤掉那些混乱的、情绪化的、非语言的细节，而这些往往是精神科或复杂疾病诊断的关键。机器懂语言，但它不懂“弦外之音”。</p>
<h3 id="5-监管全球在做什么">5. 监管：全球在做什么？</h3>
<p>世界各地的监管机构都在试图解决同一个问题：如何既要创新，又不要失控。</p>
<p><img src="https://pbs.twimg.com/media/G-jrw_ib0AAfF8W?format=jpg&#x26;name=large" alt="全球监管"></p>
<ul>
<li><strong>欧盟（预防为主）：</strong> 他们的《人工智能法案》很严格，把医疗 AI 定为“高风险”。这就像是说：“在你证明它绝对安全之前，我们要盯着你。”</li>
<li><strong>美国（风险导向）：</strong> FDA 更务实。他们把行政工具和诊断工具分开管。他们甚至搞出了“预定变更控制计划”，允许 AI 在一定范围内自我学习和进化，只要不出圈。</li>
<li><strong>日本与新加坡（国家战略）：</strong> 日本因为缺人，所以急着要用 AI，但现在开始强调国家层面的支持。新加坡则非常务实，他们建立“沙盒”，让你在可信的环境里跑，但底线很清楚：医生必须是最后的决策者。</li>
</ul>
<h3 id="6-教育的变革培养混合型医生">6. 教育的变革：培养“混合型”医生</h3>
<p><img src="https://pbs.twimg.com/media/G-jr43EbkAA8sox?format=jpg&#x26;name=large" alt="教育变革"></p>
<p>既然现实变了，我们不能再用旧方法教学生。</p>
<ul>
<li><strong>哈佛：</strong> 不再只是死记硬背解剖学，现在要学数据素养和伦理。</li>
<li><strong>英国与新加坡：</strong> 最有趣的是“无 AI 演练”。就像飞行员必须练习手动降落一样，医生必须定期被强制切断 AI 辅助，靠自己的大脑和双手看病。这不仅是为了应急，更是为了保持大脑的敏锐。</li>
</ul>
<p><img src="https://pbs.twimg.com/media/G-jsMuJXYAE1ilp?format=jpg&#x26;name=large" alt="无AI演练"></p>
<h3 id="7-结论无论是谁都不应该把大脑外包">7. 结论：无论是谁，都不应该把大脑外包</h3>
<p>对该问题的讨论，显然不光关于技术，更关于我们如何定义“医生”。</p>
<p>如果我们让 AI 取代了思考的过程，我们就不仅是在使用工具，而是在退化。未来的医生不需要和机器比记忆力，那场比赛我们已经输了。未来的医生需要做机器做不到的事：批判性地评估机器的输出，理解复杂的背景，并在机器犯傻时（它们经常犯傻）果断地介入。</p>
<p><img src="https://pbs.twimg.com/media/G-jsoE8X0AAJ0DS?format=jpg&#x26;name=large" alt="清醒的仲裁者"></p>
<p>AI 应该是望远镜，让你看得更远，但很多时候它却让人盲目前行。为了病人的福祉，医生必须是那个最终的、清醒的仲裁者。</p>
<hr>
<p><strong>关于作者</strong>
<strong>阿兹特克小羊驼</strong>
AI画匠，Prompt Kiddie、球迷（勇士&#x26;马刺@ NBA，利物浦@英超）。休憩于科技与人文的十字路口，探寻科学、思想与生活的张力。</p>13:T2b8a,<h1 id="ai与医疗效率与理解力的博弈">AI与医疗：效率与理解力的博弈</h1>
<blockquote>
<p><strong>作者</strong>：<a href="https://x.com/AztecaAlpaca">阿兹特克小羊驼 (@AztecaAlpaca)</a><br>
<strong>发布时间</strong>：2026-01-14<br>
<strong>原文链接</strong>：<a href="https://x.com/AztecaAlpaca/status/2011236247877140631">点击跳转</a></p>
</blockquote>
<p><img src="https://pbs.twimg.com/media/G-j2pjNbgAA_18n?format=jpg&#x26;name=large" alt="封面图"></p>
<h2 id="解析张文宏的去技能化预警与全球医疗系统的应对策略">解析张文宏的“去技能化”预警与全球医疗系统的应对策略</h2>
<p>在1月10日于香港举行的高山书院十周年论坛上，国家传染病医学中心（上海）主任张文宏教授就 AI 在医疗领域的应用分享了见解。</p>
<p><img src="https://pbs.twimg.com/media/G-jnyX9bQAEaYvn?format=jpg&#x26;name=large" alt="张文宏观点"></p>
<p>张文宏明确反对将其系统性地引入医院的日常诊疗流程。</p>
<blockquote>
<p><strong>张文宏解释：</strong>
他个人使用 AI 的方式是让其对病例“先看一遍”，事后凭借深厚的专业经验，他可以一看就知道 AI 哪里是错的。</p>
<p>张文宏担忧的在于，一名医生若从实习阶段就未经完整的诊断思维训练，直接借助 AI 获得结论，将导致其无法鉴别 AI 诊断的正误。这种能力的缺失，是隐藏在技术便利背后的深层隐患。</p>
</blockquote>
<p>张文宏医生是一位话题人物，经常能够抛出值得讨论的问题。这一次，无疑又引起了热议。</p>
<p><img src="https://pbs.twimg.com/media/G-jq6jUbQAMsvjl?format=jpg&#x26;name=large" alt="热议"></p>
<p>在当前，如何将 AI 应用于医疗，始终是个热点话题。首先，繁琐的医疗工作者不可能不用 AI。但医疗又是一个偏保守的行业，至少作为患者的时候，每个人还是希望能够被严肃对待。</p>
<p><img src="https://pbs.twimg.com/media/G-jtYc9bQAMC4y1?format=jpg&#x26;name=large" alt="保守与效率"></p>
<h2 id="医疗中ai-在不同阶段的参与方式与风险评估">医疗中，AI 在不同阶段的参与方式与风险评估</h2>
<p>当我们把人工智能放进医生的诊室时，我们其实是在进行一场巨大的交易。一方面，我们看到了惊人的效率——机器不知疲倦，能处理海量数据；但另一方面，我们面临着一个根本性的隐患：<strong>如果机器替医生做了所有的思考，医生还会思考吗？</strong></p>
<p><img src="https://pbs.twimg.com/media/G-jtG2wWMAAPspm?format=jpg&#x26;name=large" alt="交易与隐患"></p>
<p>这里有几个关键点：</p>
<ol>
<li><strong>认知风险是真实的：</strong> 张文宏医生提出的观点非常犀利。如果你让年轻医生在学会如何独立诊断之前就依赖 AI，你就剥夺了他们“挣扎”的机会。没有这种认知的挣扎，就没有技能的习得。最终，医生可能变成仅仅是给算法盖章的办事员。</li>
<li><strong>但收益也是巨大的：</strong> 我们不能因噎废食。在阅片这种枯燥且容易出错的工作上，AI 是完美的“第二双眼睛”。它还能处理那些让医生精疲力竭的文书工作，把医生还给病人。</li>
<li><strong>规则正在制定：</strong> 无论是美国、欧盟还是亚洲，大家都在做同一件事：给这个强大的工具装上护栏。核心原则很简单——不管机器多么聪明，最终签字负责的必须是人类。</li>
<li><strong>教育必须改变：</strong> 既然世界变了，学校也得变。哈佛和英国的医学院正在教学生如何质疑 AI，甚至强迫学生在“关掉 AI”的情况下进行演练，以确保他们的基本功没有退化。</li>
</ol>
<p>总之，AI 是一把手术刀，行善与作恶就在毫厘之间。在这一行，你必须比手中的工具更聪明，否则你就不是在做手术，而是在制造灾难。</p>
<p>接下来我们将对上述要点进行详细解读，我们需要搞清楚，为什么在医疗中，使用 AI 远比仅仅“使用工具”更复杂。</p>
<h3 id="1-便利是思维的敌人吗">1. 便利是思维的敌人吗？</h3>
<p>这就是我们面临的“认识论危机”。张文宏医生的观点触及了学习的本质。</p>
<p>想象一下学习物理，如果他在学会牛顿定律之前就只会背公式表，他永远成不了物理学家。医学也是如此。张文宏担心的是，如果我们让 AI 系统性地接管诊断流程，年轻医生就会跳过“第一性原理”的推导过程。他们会直接得到答案，而错过了那种构建直觉所必需的“脑力折磨”。</p>
<p>当 AI 直接把结果端上来时，我们的大脑就会停止那种深入的、慢速的分析（系统2思维），转而依赖直觉（系统1思维）。结果？我们得到了一群只会验证答案、不会寻找答案的医生。</p>
<p><img src="https://pbs.twimg.com/media/G-jrR_GbQAAGwcy?format=jpg&#x26;name=large" alt="思维危机"></p>
<h3 id="2-技能退化当大脑进入自动驾驶">2. 技能退化：当大脑进入“自动驾驶”</h3>
<p>这不仅仅是担忧，这已经在发生了。当你不再使用某种肌肉，它就会萎缩；大脑也是一样。</p>
<ul>
<li><strong>静默的自动驾驶：</strong> 医生习惯了 AI 总是对的，于是停止了检查。他们脑中的疾病模型因为长期不用而变得迟钝。</li>
<li><strong>捷径式学习：</strong> 实习生用 AI 读摘要，而不是自己去啃复杂的病历。这就像是只看电影解说而不看电影，你以为你懂了，其实你什么都没懂。</li>
<li><strong>提示词贫乏：</strong> 医生如果只会给 AI 下达简单的指令，他们得到的也只是泛泛的回答。</li>
</ul>
<p><img src="https://pbs.twimg.com/media/G-jrKH9bQAEp-sO?format=jpg&#x26;name=large" alt="自动驾驶风险"></p>
<p>更糟糕的是<strong>自动化偏见</strong>。这是一种心理陷阱：我们倾向于相信屏幕上的东西。即使 AI 犯了一个显而易见的错误，或者漏掉了一个关键信号，医生也可能因为过度信任机器而选择无视自己的直觉。这就像是我们为了省点脑力，就把方向盘交给了并不完美的自动驾驶仪。</p>
<h3 id="3-反方观点我们需要这个增益器">3. 反方观点：我们需要这个“增益器”</h3>
<p>尽管风险存在，但埃里克·托波尔（Eric Topol）等人的观点同样有力：人类本身就是充满缺陷的。我们会累，会看走眼，会有偏见。</p>
<p><img src="https://pbs.twimg.com/media/G-jrgedbQAMKvF8?format=jpg&#x26;name=large" alt="增益器"></p>
<ul>
<li><strong>安全网：</strong> 在放射科，AI 不累。它能发现人类因疲劳而漏掉的肺结节。这不仅是效率，这是救命。</li>
<li><strong>把时间还给医生：</strong> 现在的医生被文书工作淹没了。如果 AI 能处理这些琐事，医生就能去做只有人类能做的事——倾听病人，理解那些数据之外的痛苦。</li>
<li><strong>超越人类的极限：</strong> AI 能看见我们看不见的模式。它能综合一个人一生的健康数据来预测败血症。这不仅仅是辅助，这是超越。</li>
</ul>
<h3 id="4-深入案例ai-写病历的隐形代价">4. 深入案例：AI 写病历的隐形代价</h3>
<p>用 AI 自动生成病历听起来很棒，能省下无数小时。数据也支持这一点：医生轻松了，病人觉得医生更专注了。</p>
<p>但这里有个微妙的陷阱——<strong>“脱钩”</strong>。</p>
<p><img src="https://pbs.twimg.com/media/G-jrq8tbQAAiUxo?format=jpg&#x26;name=large" alt="脱钩"></p>
<p>写病历不仅仅是记录，它其实是医生整理思路、消化信息的过程。如果你把这个过程外包给机器，你可能就失去了对病例深层理解的机会。而且，机器往往会过滤掉那些混乱的、情绪化的、非语言的细节，而这些往往是精神科或复杂疾病诊断的关键。机器懂语言，但它不懂“弦外之音”。</p>
<h3 id="5-监管全球在做什么">5. 监管：全球在做什么？</h3>
<p>世界各地的监管机构都在试图解决同一个问题：如何既要创新，又不要失控。</p>
<p><img src="https://pbs.twimg.com/media/G-jrw_ib0AAfF8W?format=jpg&#x26;name=large" alt="全球监管"></p>
<ul>
<li><strong>欧盟（预防为主）：</strong> 他们的《人工智能法案》很严格，把医疗 AI 定为“高风险”。这就像是说：“在你证明它绝对安全之前，我们要盯着你。”</li>
<li><strong>美国（风险导向）：</strong> FDA 更务实。他们把行政工具和诊断工具分开管。他们甚至搞出了“预定变更控制计划”，允许 AI 在一定范围内自我学习和进化，只要不出圈。</li>
<li><strong>日本与新加坡（国家战略）：</strong> 日本因为缺人，所以急着要用 AI，但现在开始强调国家层面的支持。新加坡则非常务实，他们建立“沙盒”，让你在可信的环境里跑，但底线很清楚：医生必须是最后的决策者。</li>
</ul>
<h3 id="6-教育的变革培养混合型医生">6. 教育的变革：培养“混合型”医生</h3>
<p><img src="https://pbs.twimg.com/media/G-jr43EbkAA8sox?format=jpg&#x26;name=large" alt="教育变革"></p>
<p>既然现实变了，我们不能再用旧方法教学生。</p>
<ul>
<li><strong>哈佛：</strong> 不再只是死记硬背解剖学，现在要学数据素养和伦理。</li>
<li><strong>英国与新加坡：</strong> 最有趣的是“无 AI 演练”。就像飞行员必须练习手动降落一样，医生必须定期被强制切断 AI 辅助，靠自己的大脑和双手看病。这不仅是为了应急，更是为了保持大脑的敏锐。</li>
</ul>
<p><img src="https://pbs.twimg.com/media/G-jsMuJXYAE1ilp?format=jpg&#x26;name=large" alt="无AI演练"></p>
<h3 id="7-结论无论是谁都不应该把大脑外包">7. 结论：无论是谁，都不应该把大脑外包</h3>
<p>对该问题的讨论，显然不光关于技术，更关于我们如何定义“医生”。</p>
<p>如果我们让 AI 取代了思考的过程，我们就不仅是在使用工具，而是在退化。未来的医生不需要和机器比记忆力，那场比赛我们已经输了。未来的医生需要做机器做不到的事：批判性地评估机器的输出，理解复杂的背景，并在机器犯傻时（它们经常犯傻）果断地介入。</p>
<p><img src="https://pbs.twimg.com/media/G-jsoE8X0AAJ0DS?format=jpg&#x26;name=large" alt="清醒的仲裁者"></p>
<p>AI 应该是望远镜，让你看得更远，但很多时候它却让人盲目前行。为了病人的福祉，医生必须是那个最终的、清醒的仲裁者。</p>
<hr>
<p><strong>关于作者</strong>
<strong>阿兹特克小羊驼</strong>
AI画匠，Prompt Kiddie、球迷（勇士&#x26;马刺@ NBA，利物浦@英超）。休憩于科技与人文的十字路口，探寻科学、思想与生活的张力。</p>8:["$","main",null,{"className":"min-h-screen pb-20","children":[["$","$L10",null,{}],["$","$L11",null,{"contentHtml":"$12"}],["$","article",null,{"className":"container-custom relative md-theme-amp-outer","children":[["$","header",null,{"className":"mb-10","children":[["$","h1",null,{"className":"text-3xl md:text-4xl font-bold mb-4 text-center font-serif text-[#3e3832] dark:text-[#f0f6fc]","children":"AI与医疗：效率与理解力的博弈"}],["$","p",null,{"className":"text-xl text-muted mb-4","children":"解析张文宏的“去技能化”预警与全球医疗系统的应对策略"}],["$","div",null,{"className":"text-sm text-muted","children":"2026-01-14"}]]}],["$","div",null,{"className":"prose md-theme-amp","dangerouslySetInnerHTML":{"__html":"$13"}}],false,["$","div",null,{"className":"mt-16 pt-8 border-t border-[var(--border-subtle)]","children":["$","$L14",null,{"href":"/articles","className":"text-accent hover:text-foreground transition-colors","children":"← Back to Articles"}]}]]}]]}]
e:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
c:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"AI与医疗：效率与理解力的博弈 - AlpacaNotes"}],["$","meta","2",{"name":"description","content":"在1月10日于香港举行的高山书院十周年论坛上，国家传染病医学中心（上海）主任张文宏教授就 AI 在医疗领域的应用分享了见解。"}],["$","meta","3",{"property":"og:title","content":"AI与医疗：效率与理解力的博弈 - AlpacaNotes"}],["$","meta","4",{"property":"og:description","content":"在1月10日于香港举行的高山书院十周年论坛上，国家传染病医学中心（上海）主任张文宏教授就 AI 在医疗领域的应用分享了见解。"}],["$","meta","5",{"property":"og:url","content":"https://linguista.cn/alpacanotes/articles/AI-and-healthcare-2026-zhang"}],["$","meta","6",{"property":"og:image","content":"https://linguista.cn/alpacanotes/aztecaalpaca.jpg"}],["$","meta","7",{"property":"og:type","content":"article"}],["$","meta","8",{"property":"article:published_time","content":"2026-01-14"}],["$","meta","9",{"property":"article:author","content":"@linguista2025"}],["$","meta","10",{"property":"article:tag","content":"AI"}],["$","meta","11",{"property":"article:tag","content":"医疗"}],["$","meta","12",{"property":"article:tag","content":"张文宏"}],["$","meta","13",{"property":"article:tag","content":"人工智能"}],["$","meta","14",{"property":"article:tag","content":"医疗系统"}],["$","meta","15",{"name":"twitter:card","content":"summary"}],["$","meta","16",{"name":"twitter:title","content":"AI与医疗：效率与理解力的博弈 - AlpacaNotes"}],["$","meta","17",{"name":"twitter:description","content":"在1月10日于香港举行的高山书院十周年论坛上，国家传染病医学中心（上海）主任张文宏教授就 AI 在医疗领域的应用分享了见解。"}],["$","meta","18",{"name":"twitter:image","content":"https://linguista.cn/alpacanotes/aztecaalpaca.jpg"}],["$","link","19",{"rel":"icon","href":"/alpacanotes/favicon.svg","type":"image/svg+xml"}]]
a:null
